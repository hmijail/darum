# Dafny Resource Usage Measurement

## What does Darum do?

Dafny verifies code by translating it into Assertion Batches in the Boogie language, that then are verified by the Z3 solver.

For a long time, the common advice to make a piece of Dafny code pass verification was to add information to help Z3 find a solution. However, since Dafny 3.x there’s been a growing effort by the Dafny developers to add facilities to control what information reaches the Z3 solver. This is because the solver can sometimes have "too much information" and go down unproductive paths while looking for a solution. This bogs down the development process by causing longer verification times or timeouts instead of definite answers, and is a very common pain point in Dafny (and the wider Z3 / SMT solver ecosystem).

Managing what the solver “knows” tantamounts to guiding it down the right path by removing alternatives. The problem then is that Dafny's automation (compared to interactive theorem provers) inherently makes it difficult to know at any point what is actually the right path and what does the solver actually know. Indeed, at the Dafny level (that is, without digging into Boogie and Z3), the only information we get about the process is the result (verification successful, failure or timeout), plus how costly was it for the solver to reach that result, measured in arbitrary units of "Resource Usage".

Darum helps the user find patterns in the solver costs, discovering hints to guide the debugging of verification brittleness. It does so by analysing and comparing the cost of verification at different granularities of the Assertion Batches, in a mostly automated way, by using existing Dafny facilities.


## Terminology

* Dafny Assertions: Dafny internally works with both explicit assertions (those in the code) and implicit ones (generated internally from the code). For clarity, we'll refer to both of them as DAs.
* AB: Assertion Batch. The way in which DAs are grouped by Dafny for verification. They are the minimal unit of observable behavior in Dafny's results.
* Brittleness: A verification result is said to be brittle when its cost isn't stable: an unchanging Dafny file can happen to always verify correctly, but with wildly different Resource Usage. [^1]
* IA: Isolate Assertions mode. During standard verification, Dafny groups all DAs generated by a member into a single Assertion Batch. A radical alternative is to use the `--isolate-assertions` argument, which causes Dafny to isolate each DA into a separate Assertion Batch. This is the finest verification granularity that Dafny offers.
* Member: Dafny's methods or functions
* RU: Resource Usage: the cost of a verification run, as reported by Z3. RU costing is more deterministic than timing-based costing.
* OoR: Out of Resources. It's the result when Dafny/Boogie/Z3 are given a resource limit and the verification takes more RU than this limit. Equivalent to a timeout.

[^1]: It's worth noting that Dafny can also flip-flop between explicit success and failure of verification. In this case, the official advice is to believe the success result but work on fixing the failures. [link]

## How does Darum work?

The key insights that Darum exploits are:
* The RU needed to verify any Dafny code pertains to a probability distribution of evolving shape.
* These distributions tend to grow wide and multimodal, causing the user to think that some problem appears and disappears – as opposed to a cost varying across a smooth range.
* The distributions of ABs compound to members' distributions worse than linearly, hence fixing narrow distributions in ABs can have outsized effects higher up.

### An evolving probability distribution?

Consider the evolution of a piece of Dafny code:
* When the code is simple, the distribution is close to a spike: every verification run returns a predictable value.
* As the code grows and turns more complicated, the distribution starts to widen, and so time/RU needed for verification starts to vary. Perversely, this is hard to notice initially because probably the worst case is still fast enough that the user doesn't stop to think about it: the code is growing, so growing verification time is in principle to be expected. Furthermore, in a bigger codebase in which individual members are only starting to misbehave, the total distribution tends to smooth and statistically compensate for the individual variation.
* At some point, some AB/member's verification gets complex enough that its distribution turns multimodal: sometimes verification runs fast, sometimes it runs much more slowly. Even worse, when this happens in one AB/member, because of how Dafny + Boogie + Z3 work internally, that randomness will keep affecting the total distribution even while working on other ABs/members.
* As work progresses, other members' distributions will keep widening and also turn multimodal, each of them affecting the total distribution with new modes.

The final result is that one starts editing line X of a Dafny file and suddenly verification fails in a surprising, seemingly unrelated way. Undoing the latest changes might seem to restore the good behavior - but this is an illusion, the randomness was there anyway. In fact, redoing the latest changes now seems to work well after all! One shrugs the problem off and pushes forward, but 2 lines later again the failure appears. *One can't pinpoint why changes to a member sometimes cause a timeout but other times verify correctly; what seemed a stable configuration suddenly stops working even though everything seems to be the same. After some busywork suddenly things seem OK, so again one pushes forward... until next stop.*

It's worth noting the unfortunate similarity of this situation to that of intermittent conditioning in a Skinner box: random actions seem to trigger unexplained positive outcomes, causing the subject to develop superstition-like behaviors, with the hope of triggering further positive outcomes.

### Why would a cheap verification turn expensive?

The fact that a cheap verification exists at all is great news: if we managed to keep Z3 in that verification path every time, we'd have fast and stable verification.

The problem is that Z3 randomly follows a different path each time, and so if there's unneeded, "distracting" information, Z3 is bound to eventually follow it, possibly getting lost. This unneeded information comes from other DAs introduced by the context built up by the containing member. Hence **reducing the unneeded information introduced into an AB by previous DAs is key to stop Z3 from getting lost.**

IA mode breaks down a member's standard single AB into as many ABs as DAs. E.g., if member M contains DA1 and DA2, AB2 would be equivalent to `assume DA1; assert DA2`.

One intriguing result of using IA mode is that the sum of the cost of the isolated ABs in a member is *typically* much more expensive than verifying the whole member as a single AB, but also much more stable. This suggests 2 possibilities:
  - Stabilisation by pessimisation: break down every member into smaller members [^2].
  - Conversely, sometimes ABs require higher RU than the containing member, or even fail to verify. This likely means that previous DAs in the member built up some context that helped / was necessary for the current DA to pass verification. Notably, this context includes facts that the solver “discovered” while proving previous DAs, and which will be missing when those DAs are `assume`d. It’s **a case where DAs grouped into an AB help each other**.

Consider that, as code evolves, DAs (both explicit and implicit) change, creating different possible paths for the solver. A member whose DAs support each other but don't unduly widen the horizon would then create a robust path, resilient to small changes. In contrast, a member whose DAs only tangentially build on each other and that widen the horizon unnecessarily will be vulnerable, or even prone, to a wide variation of costs.

While the first impulse might be to limit the length of members, and this would help in a way, note that the length is not the real problem. The key consideration is whether the DAs in the member build on each other robustly.

[^2]: Either by physically defining new members, or using facilities like {:split_here} or IA mode itself.

### So how does Darum help?

Darum can be used to:
1. Analyse cost and variability of verification at various AB granularities.
   * Standard verification mode: Simply running multiple verifications on a Dafny file is enough to discover whether the cost is stable or not. Typically, a Dafny file will contain multiple members, so Darum helps decide which ones will yield the greatest gains if stabilized.
   * Isolated Assertions mode: The extreme case of turning every DA into a new AB.
   * Anything in between: While IA mode is enabled by a simple CLI argument, the Dafny programmer can also manually break down a member into multiple ABs by either writing smaller members, or using facilities like `{:split_here}`.
2. Compare cost and variability of verifying at different granularities.
    * Knowing how the distribution of verification costs varies across different AB granularities can point to stabilization opportunities, and even to what needs to be done for stabilization.


## What exactly is Darum?

Darum consists of 3 loosely coupled tools:
* `dafny_measure`: a tool to drive `dafny measure-complexity` in a way that eases management of the generated logs, by recording:
  - The timestamp
  - The arguments used
  - part of the input file's hash, to ensure that multiple logs can be meaningfully compared

  Additionally, `dafny_measure` warns if `dafny` misbehaves, like when z3 processes are leaked (bug XXX).
* `plot_distribution`: a tool to be run on the logs generated by `dafny measure-complexity`. It runs some tests on the verification results contained in the log, scores them for improvement potential, presents the results in summary tables, and plots the most interesting results for further analysis.
* `compare_distribution`: a tool to compare verification runs with and without "Isolate Assertions" mode.

## Installation

Darum's tools are written in Python and available in Pypi.

Probably the easiest way to install Darum is using `pipx`, which should be available in all package managers, like `brew` in MacOS.

```
$ brew install pipx
...
$ pipx install darum
```

## Usage

Each of the tools has a `--help` argument that lists the available options.

```
$ dafny_measure myfile.dfy
...
$ plot_distribution logfile.log
...
$ dafny_measure myfile.dfy --isolate-assertions -e"--filter-symbol expensiveFunction"
...
$ compare_distribution logfile.log -i logfile_IA
...
```




## Interpreting the results

### The plots

#### Plain plots

##### Worst offenders

ABs are scored according to their characteristics, including the fact that a non-successful AB makes following ABs inside the same member unreliable.

The top N are plotted. For plots with failures/OoRs, the rightmost bar is wider to highlight those failures.

The plot starts in transparent mode to make it easier to see where bars overlap. Clicking on the legend makes the corresponding plot easier to see.

Verification results that happen rarely are specially important. Hence, the Y axis is logarithmic to more easily capture single atypical results.


#### Comparative plots

### The table/s

### Comments

### General discussion

Rule of thumb: isolated assertions: AB have span <3%. Full funcs/methods < 10%.

Dafny's official [docs](https://dafny.org/dafny/DafnyRef/DafnyRef.html#sec-brittle-verification) and [tools](https://github.com/dafny-lang/dafny-reportgenerator/blob/main/README.md) use statistical measures like stddev and RMS% to measure verification brittleness. However, we argue that it's more useful to think of simple min/max values. For example, consider the case of running 10 iterations of the verification process, in which 9 of the results are closely clustered but one single result deviates far away, being either much cheaper or much more expensive than the rest. Taking the stddev or RMS of these 10 cases would dampen the extremes, while we argue that they are precious hints that needs to be highlighted instead. Indeed, each time that the verification runs, these rare but extreme values are the ones with potential to turn things unexpectedly slow or fast. Furthermore, AB variability seems to compose disproportionally into more extreme variability at the member level, multiplying the effect of AB's span. This all suggests that, for reliability, it's necessary to minimize the span of Resource Usage costs.

It's worth noting that, while we're focusing on RU variability to combat brittleness, these tools are also useful to account for plain RU usage and rank where the verification time is being spent in the code.

## Some remedies to keep in mind

### Dafny library

### Section on Verification debugging in Ref Manual

